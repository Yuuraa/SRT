# SRT
> [**i-SRT:Aligning Large Multimodal Models for Videos by Iterative Self-Retrospective Judgement**](),            
[Daechul Ahn*](https://dcahn12.github.io)<sup>1,3</sup>,
[Yura Choi*](https://yuuraa.github.io)<sup>1,3</sup>,
San Kim<sup>1,3</sup>,
[Youngjae Yu](https://yj-yu.github.io/home/)<sup>1</sup>, 
[Dongyeop Kang](https://dykang.github.io)<sup>2</sup>,
[Jonghyun Choi](https://ppolon.github.io)<sup>3,&dagger;</sup>(*Equal Contribution)<br>
<sup>1</sup>Yonsei University,
<sup>2</sup>University of Minnesota,
<sup>3</sup>Seoul National University<br>
<sup>&dagger;</sup>Corresponding Author<br>

> **Abstract:** *Aligning Video Large Multimodal Models (VLMMs) face challenges such as modality misalignment and verbose responses. Although iterative approaches such as self-rewarding or iterative direct preference optimization (DPO) recently showed a significant improvement in language model alignment, particularly on reasoning tasks, self-aligned models applied to large video-language models often result in lengthy and irrelevant responses. To address these challenges, we propose a novel method that employs self-retrospection to enhance both response generation and preference modeling, and call iterative self-retrospective judgment (i-SRT). By revisiting and evaluating already generated content and preference in loop, i-SRT improves the alignment between textual and visual modalities, reduce verbosity, and enhances content relevance. Our empirical evaluations across diverse video question answering benchmarks demonstrate that i-SRT significantly outperforms prior arts. We are committed to opensourcing our code, models, and datasets to encourage further investigation.*

## Approach
![Overview](images/overview.png)


Code is Coming Soon!

## Release
- [06/17] Create repository, update README

## Building Preference Data w/ Model

## Training

## Evaluation




## Acknowledgement
- [LLaVA-Hound-DPO](): Our code is built upon the codebase from LLaVA-Hound-DPO